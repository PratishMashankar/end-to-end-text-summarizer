# creates an artifacts folder where all generation from components is stored
artifacts_root: artifacts

# downloads the data from source_url inside the root_dir as local_data_file unzipped into unzip_dir
# updates in the data path here are reflected in all the files since these are variables
data_ingestion:
  root_dir: artifacts/data_ingestion
  source_URL: https://github.com/entbappy/Branching-tutorial/raw/master/summarizer-data.zip
  local_data_file: artifacts/data_ingestion/data.zip
  unzip_dir: artifacts/data_ingestion


# check if the three files are available or not
data_validation:
  root_dir: artifacts/data_validation
  STATUS_FILE: artifacts/data_validation/status.txt
  ALL_REQUIRED_FILES: ["train", "test", "validation"]


# save transformed data in a new folder
data_transformation:
  root_dir: artifacts/data_transformation
  data_path: artifacts/data_ingestion/samsum_dataset
  tokenizer_name: google/pegasus-cnn_dailymail


# save the model after training
model_trainer:
  root_dir: artifacts/model_trainer
  data_path: artifacts/data_transformation/samsum_dataset
  model_ckpt: google/pegasus-cnn_dailymail



# model_evaluation:
#   root_dir: artifacts/model_evaluation
#   data_path: artifacts/data_transformation/samsum_dataset
#   model_path: artifacts/model_trainer/pegasus-samsum-model
#   tokenizer_path: artifacts/model_trainer/tokenizer
#   metric_file_name: artifacts/model_evaluation/metrics.csv